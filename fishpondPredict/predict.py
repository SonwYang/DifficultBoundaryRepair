#!/usr/bin/env python
# coding: utf-8
import cv2
from path import Path
from dataset import SasDataset
import torch
import torch.backends.cudnn
from torch.utils.data import DataLoader
from torch.autograd import Variable
import torch.nn.functional as F
from osgeo import osr, ogr
import numpy as np
import shutil
import glob
import tqdm
import os
from scipy import ndimage as ndi
from config_eval import ConfigEval
from skimage.morphology import remove_small_holes, closing, square, opening, remove_small_objects, watershed, label
from skimage import io
from shutil import rmtree
import warnings
from osgeo import gdal
from geoClip import array_to_raster
warnings.filterwarnings("ignore")

# need to create a file to store temp pictures
path = './temp_pic/'

device = 'cuda:0'
dark = [0, 0, 0]

def read_img(filename):
    dataset=gdal.Open(filename)

    im_width = dataset.RasterXSize
    im_height = dataset.RasterYSize

    im_geotrans = dataset.GetGeoTransform()
    im_proj = dataset.GetProjection()
    im_data = dataset.ReadAsArray(0,0,im_width,im_height)

    del dataset
    return im_proj, im_geotrans, im_width, im_height, im_data


def write_img(filename, im_proj, im_geotrans, im_data):
    if 'int8' in im_data.dtype.name:
        datatype = gdal.GDT_Byte
    elif 'int16' in im_data.dtype.name:
        datatype = gdal.GDT_UInt16
    else:
        datatype = gdal.GDT_Float32

    if len(im_data.shape) == 3:
        im_bands, im_height, im_width = im_data.shape
    else:
        im_bands, (im_height, im_width) = 1,im_data.shape

    driver = gdal.GetDriverByName("GTiff")
    dataset = driver.Create(filename, im_width, im_height, im_bands, datatype)

    dataset.SetGeoTransform(im_geotrans)
    dataset.SetProjection(im_proj)

    if im_bands == 1:
        dataset.GetRasterBand(1).WriteArray(im_data)
    else:
        for i in range(im_bands):
            dataset.GetRasterBand(i+1).WriteArray(im_data[i])

    del dataset

# 水平翻转
def flip_horizontal_tensor(batch):
    columns = batch.data.size()[-1]
    return batch.index_select(-1, torch.LongTensor(list(reversed(range(columns)))).to(device))


#   垂直翻转
def flip_vertical_tensor(batch):
    rows = batch.data.size()[-2]
    return batch.index_select(-2, torch.LongTensor(list(reversed(range(rows)))).to(device))

# 水平翻转
def flip_horizontal_tensor_cpu(batch):
    columns = batch.data.size()[-1]
    return batch.index_select(-1, torch.LongTensor(list(reversed(range(columns)))))

#   垂直翻转
def flip_vertical_tensor_cpu(batch):
    rows = batch.data.size()[-2]
    return batch.index_select(-2, torch.LongTensor(list(reversed(range(rows)))))


def image_normalization(img, img_min=0, img_max=255,
                        epsilon=1e-12):
    """This is a typical image normalization function
    where the minimum and maximum of the image is needed
    source: https://en.wikipedia.org/wiki/Normalization_(image_processing)

    :param img: an image could be gray scale or color
    :param img_min:  for default is 0
    :param img_max: for default is 255

    :return: a normalized image, if max is 255 the dtype is uint8
    """

    img = np.float32(img)
    # whenever an inconsistent image
    img = (img - np.min(img)) * (img_max - img_min) / \
        ((np.max(img) - np.min(img)) + epsilon) + img_min
    return img


def normal_img(img):
    min = np.min(img)
    max = np.max(img)
    img = (img - min) / (max - min) * 255
    return img.astype(np.uint8)


def read_geoimg(path):
    data = gdal.Open(path)
    lastChannel = data.RasterCount + 1
    arr = [normal_img(data.GetRasterBand(idx).ReadAsArray()) for idx in range(1, lastChannel)]
    arr = np.dstack(arr)
    return arr[:, :, :3]


def input_and_output(pic_path, model, cfg, loader=None, generate_data=True, scale_size=None):
    """
    args:
        pic_path : the picture you want to predict
        model    : the model you want to predict
    note:
        step one : generate some pictures from one picture
        step two : predict from the images generated by step one 
    """
    image_size = cfg.crop_size

    data = gdal.Open(pic_path)
    lastChannel = data.RasterCount + 1
    #arr = [normal_img(data.GetRasterBand(idx).ReadAsArray()) for idx in range(1, lastChannel)]
    arr = [data.GetRasterBand(idx).ReadAsArray() for idx in range(1, lastChannel)]
    data = np.dstack(arr)

    raw_h, raw_w = data.shape[:2]

    # data = cv2.bilateralFilter(data, 9, 75, 75)

    b = cfg.padding_size
    row = raw_h // image_size + 1
    col = raw_w // image_size + 1
    radius_h = row * image_size - raw_h
    radius_w = col * image_size - raw_w
    data = cv2.copyMakeBorder(data, 0, radius_h, 0, radius_w, cv2.BORDER_REFLECT)

    data = cv2.copyMakeBorder(data, b, b, b, b, cv2.BORDER_REFLECT)

    h, w = data.shape[:2]

    # padding_img = data[:, :, :]

    data = np.array(data)
    mask_whole = np.zeros((row*image_size, col*image_size), dtype=np.uint8)
    mask_whole_dn = np.zeros((row*image_size, col*image_size), dtype=np.uint8)

    if generate_data == False:
        print('starting prediction')
        result = []
        result_dn = []
        for batch in tqdm.tqdm(loader):
            # images = batch['img'].to(device, dtype=torch.float)
            images = Variable(batch['img'].to(device, dtype=torch.float))
            h, w = images.shape[2:]
            new_h = int(h * scale_size)
            images = F.interpolate(images, size=(new_h, new_h), mode='bilinear')
            temp = 0
            temp_dn = 0
            for keys in model.keys():
                # model[keys].eval()
                net = model[keys]
                net.eval()
                if cfg.TTA:
                    # outputs1, _ = net(images)
                    # outputs1 = outputs1.cpu().detach()
                    # outputs2, _ = net(flip_horizontal_tensor(images))
                    # outputs2 = flip_horizontal_tensor(outputs2).cpu().detach()

                    outputs1 = net(images).cpu().detach()
                    outputs2 = flip_horizontal_tensor(net(flip_horizontal_tensor(images))).cpu().detach()
                    # outputs3 = flip_vertical_tensor(net(flip_vertical_tensor(images))).cpu().detach()

                    outputs = (outputs2 + outputs1)/2.

                else:
                    outputs = net(images, seg=True)

                prob = F.softmax(outputs, dim=1)
                prob = prob[0]
                _, maxprob = torch.max(prob, 0)

                # prob = prob[0, 1].numpy()
                # maxprob = np.where(prob > 0.1, 1, 0)

                fuse = maxprob

                prob = torch.sigmoid(outputs).numpy()
                tmp_img = prob[0, 1, ...]
                tmp_img = np.uint8(image_normalization(tmp_img)).squeeze()
                tmp_img = cv2.bitwise_not(tmp_img)

                temp += fuse
                temp_dn += tmp_img
            preds = temp / len(model)
            # preds = cv2.resize(preds, (h, w))
            # preds = torch.from_numpy(preds)
            result.append(preds)
            result_dn.append(temp_dn)

        map_list = [str(i.name) for i in Path('temp_pic').files()]
    for i in tqdm.tqdm(range(row)):
        for j in range(col):
            if generate_data:
                crop_img = redundancy_crop(data, i, j, image_size, cfg)

                array_to_raster(crop_img.astype(np.uint8), f'temp_pic/{i}_{j}.tif')
                #io.imsave(f'temp_pic/{i}_{j}.png', crop_img.astype(np.uint8))
                # cv2.imwrite(f'temp_pic/{i}_{j}.tif', crop_img)
            else:
                temp = result[map_list.index(f'{i}_{j}.tif')]
                temp = redundancy_crop2(temp, cfg)
                temp_dn = result_dn[map_list.index(f'{i}_{j}.tif')]
                temp_dn = redundancy_crop2(temp_dn, cfg)
                mask_whole[i * image_size:i * image_size + image_size,
                j * image_size:j * image_size + image_size] = temp
                mask_whole_dn[i * image_size:i * image_size + image_size,
                j * image_size:j * image_size + image_size] = temp_dn
    return mask_whole[:raw_h, :raw_w], mask_whole_dn[:raw_h, :raw_w]


def redundancy_crop(img, i, j, targetSize, cfg):
    if len(img.shape)>2:
        temp_img = img[i*targetSize:i*targetSize+targetSize+2*cfg.padding_size, j*targetSize:j*targetSize+targetSize+2*cfg.padding_size, :]
    else:
        temp_img = img[i*targetSize:i*targetSize+targetSize+2*cfg.padding_size, j*targetSize:j*targetSize+targetSize+2*cfg.padding_size]
    return temp_img


def redundancy_crop2(img, cfg):
    h = img.shape[0]
    w = img.shape[1]
    temp_img = img[cfg.padding_size:h-cfg.padding_size, cfg.padding_size:w-cfg.padding_size]
    return temp_img


def get_dataset_loaders():
    batch_size = 1

    test_dataset = SasDataset(
        "./temp_pic",
        mode='test'
    )

    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)
    return test_loader


def get_labels():
    """Load the mapping that associates pascal classes with label colors

    Returns:
        np.ndarray with dimensions (2, 3)
    """
    return np.asarray(
        [
            [0, 0, 0],
            [255, 255, 255]
        ]
    )


def decode_segmap(label_mask, n_classes):
    """Decode segmentation class labels into a color image

    Args:
        label_mask (np.ndarray): an (M,N) array of integer values denoting
          the class label at each spatial location.
        plot (bool, optional): whether to show the resulting color image
          in a figure.

    Returns:
        (np.ndarray, optional): the resulting decoded color image.
    """
    label_colours = get_labels()
    r = label_mask.copy()
    g = label_mask.copy()
    b = label_mask.copy()
    for ll in range(0, n_classes):
        r[label_mask == ll] = label_colours[ll, 0]
        g[label_mask == ll] = label_colours[ll, 1]
        b[label_mask == ll] = label_colours[ll, 2]
    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))
    rgb[:, :, 0] = r
    rgb[:, :, 1] = g
    rgb[:, :, 2] = b
    return rgb


###mask1   big    mask2 small
def my_watershed(mask1, mask2):
    """
    watershed from mask1 with markers from mask2
    """
    markers = ndi.label(mask2, output=np.uint32)[0]
    labels = watershed(mask1, markers, mask=mask1, watershed_line=True)
    return labels


def mkdir(path):
    if not os.path.exists(path):
        os.mkdir(path)


def predict_seg(cfg):
    # 递归删除文件夹
    try:
        shutil.rmtree('temp_pic')
    except:
        pass

    save_path = cfg.save_path
    if os.path.exists(save_path):
        rmtree(save_path)

    mkdir(save_path)
    # model_groups = glob.glob(cfg.model_path+'/*.pth')
    model_groups = [cfg.seg_model_path]
    # imgList = glob.glob(cfg.data_path + '/*.tif')
    imgList = [cfg.data_path]
    num = len(imgList)

    # predict on more model
    print('loading models')
    models = {}
    for index, item in enumerate(model_groups):

        models[item] = torch.load(item, map_location=device)
        # models[item].load_segnet(r'D:\2021\3\EESNet_test\model_results\epoch253_model.pth', device)
        # models[item] = torch.load(item, map_location='cuda:0')["ema_state_dict"]

    # model = torch.load(f'./results_{args.model_name}/{args.model_name}_weights_best.pth')["model_state"]

    for i in tqdm.tqdm(range(num)):
        print(f"starting processing {imgList[i]}")
        im_proj, im_geotrans, im_width, im_height, im_data = read_img(imgList[i])
        if not os.path.exists('temp_pic'):
            os.makedirs('temp_pic')

        input_and_output(imgList[i], models, cfg, generate_data=True)

        for j, scale_size in enumerate([1]):
        # for j, scale_size in enumerate([0.75, 0.875, 1]):

            name = os.path.split(imgList[i])[-1].split(".")[0]
            test_loader = get_dataset_loaders()
            mask_result, mask_result_dn = input_and_output(imgList[i], models, cfg, loader=test_loader,
                                                           generate_data=False, scale_size=scale_size)
            mask_result = mask_result.astype(np.uint8)
            mask_result_dn = mask_result_dn.astype(np.uint8)
            mask_result = remove_small_holes(mask_result, 400)
            mask_result = remove_small_objects(mask_result, 144)
            # mask_result = np.where(mask_result > 0, 1, 0)

            write_img(os.path.join(save_path, name + f'.tif'), im_proj, im_geotrans, mask_result * 255)
            write_img(os.path.join(save_path, name + f'_dn.tif'), im_proj, im_geotrans, 255 - mask_result_dn)

        # 递归删除文件夹
        try:
            shutil.rmtree('temp_pic')
        except:
            pass


def area(shpPath):
    '''计算面积'''
    driver = ogr.GetDriverByName("ESRI Shapefile")
    dataSource = driver.Open(shpPath, 1)
    layer = dataSource.GetLayer()

    src_srs = layer.GetSpatialRef()
    tgt_srs = osr.SpatialReference()
    tgt_srs.ImportFromEPSG(32649)
    transform = osr.CoordinateTransformation(src_srs, tgt_srs)
    # geosr.SetWellKnownGeogCS("WGS_1984_UTM_Zone_49N")

    new_field = ogr.FieldDefn("Area", ogr.OFTReal)
    new_field.SetWidth(32)
    new_field.SetPrecision(16)
    layer.CreateField(new_field)
    for feature in layer:
        geom = feature.GetGeometryRef()
        geom2 = geom.Clone()
        geom2.Transform(transform)

        area_in_sq_m = geom2.GetArea()
        # area_in_sq_km = area_in_sq_m / 1000000

        feature.SetField("Area", area_in_sq_m)
        layer.SetFeature(feature)


## python predict.py --data_path ${data_path} --seg_model_path ${coarse segmentation model}
if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='''This is a code for prediction.''')
    parser.add_argument('--data_path', type=str, default=r'F:\2022\5\nanliu\20100927_nanliucun\20100927_nanliucun\L21\20100927_nanliucun.tif',
                        help='path to the checkpoint')
    parser.add_argument('--seg_model_path', type=str,
                        default=r"F:\paper\fishpond\fishpondCode\fishpondTrain\ckpts_unet_water\epoch0224_model.pth",
                        help='network input layer height size')
    args = parser.parse_args()

    pwd = os.path.dirname(os.path.abspath(__file__))

    cfg = ConfigEval()

    cfg.data_path = args.data_path
    cfg.seg_model_path = args.seg_model_path

    predict_seg(cfg)

    # model = torch.load(r'D:\MyWorkSpace\paper\fishpond\fishpondSeg\ckpts_finshpond8\epoch257_model.pth',  map_location="cuda:0")
    # input = torch.randn((2, 8, 256, 256)).cuda()
    # output = model(input)
    # print(output.size())

